{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6.2 Neural network for Regression.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCntI323FpHA",
        "outputId": "4252cd9e-2d86-4bae-f54e-7efa68b79fdb"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "df = pd.read_csv('/content/heart_failure_clinical_records_dataset.csv')\n",
        "\n",
        "class df(torch.utils.data.Dataset):\n",
        "#Prepare the dataset for regression\n",
        "\n",
        "\n",
        "  def __init__(self, X, y, scale_data=True):\n",
        "    if not torch.is_tensor(X) and not torch.is_tensor(y):\n",
        "      if scale_data:\n",
        "          X = StandardScaler().fit_transform(X)\n",
        "      self.X = torch.from_numpy(X)\n",
        "      self.y = torch.from_numpy(y)\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.X)\n",
        "\n",
        "  def __getitem__(self, i):\n",
        "      return self.X[i], self.y[i]\n",
        "      \n",
        "\n",
        "class MLP(nn.Module):\n",
        "    #Multilayer Perceptron for regression.\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "      nn.Linear(13, 64),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(64, 32),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(32, 1)\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "      #Forward pass\n",
        "\n",
        "    return self.layers(x)\n",
        "\n",
        "  \n",
        "if __name__ == '__main__':\n",
        "  \n",
        "  # Set fixed random number seed\n",
        "  torch.manual_seed(42)\n",
        "  \n",
        "  from sklearn.datasets import load_boston\n",
        "  # Load Boston dataset\n",
        "  X, y = load_boston(return_X_y=True)\n",
        "  \n",
        "  # Prepare Boston dataset\n",
        "  dataset = df(X, y)\n",
        "  trainloader = torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=True, num_workers=1)\n",
        "  \n",
        "  # Initialize the MLP\n",
        "  mlp = MLP()\n",
        "  \n",
        "  # Define the loss function and optimizer\n",
        "  loss_function = nn.L1Loss()\n",
        "  optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)\n",
        "  \n",
        "  # Run the training loop\n",
        "  for epoch in range(0, 5): # 5 epochs at maximum\n",
        "    \n",
        "    # Print epoch\n",
        "    print(f'Starting epoch {epoch+1}')\n",
        "    \n",
        "    # Set current loss value\n",
        "    current_loss = 0.0\n",
        "    \n",
        "    # Iterate over the DataLoader for training data\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "      \n",
        "      # Get and prepare inputs\n",
        "      inputs, targets = data\n",
        "      inputs, targets = inputs.float(), targets.float()\n",
        "      targets = targets.reshape((targets.shape[0], 1))\n",
        "      \n",
        "      # Zero the gradients\n",
        "      optimizer.zero_grad()\n",
        "      \n",
        "      # Perform forward pass\n",
        "      outputs = mlp(inputs)\n",
        "      \n",
        "      # Compute loss\n",
        "      loss = loss_function(outputs, targets)\n",
        "      \n",
        "      # Perform backward pass\n",
        "      loss.backward()\n",
        "      \n",
        "      # Perform optimization\n",
        "      optimizer.step()\n",
        "      \n",
        "      # Print statistics\n",
        "      current_loss += loss.item()\n",
        "      if i % 10 == 0:\n",
        "          print('Loss after mini-batch %5d: %.3f' %\n",
        "                (i + 1, current_loss / 500))\n",
        "          current_loss = 0.0\n",
        "\n",
        "  # Process is complete.\n",
        "  print('Training process has finished.')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
            "\n",
            "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
            "    the documentation of this function for further details.\n",
            "\n",
            "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
            "    dataset unless the purpose of the code is to study and educate about\n",
            "    ethical issues in data science and machine learning.\n",
            "\n",
            "    In this special case, you can fetch the dataset from the original\n",
            "    source::\n",
            "\n",
            "        import pandas as pd\n",
            "        import numpy as np\n",
            "\n",
            "\n",
            "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
            "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
            "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
            "        target = raw_df.values[1::2, 2]\n",
            "\n",
            "    Alternative datasets include the California housing dataset (i.e.\n",
            "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
            "    dataset. You can load the datasets as follows::\n",
            "\n",
            "        from sklearn.datasets import fetch_california_housing\n",
            "        housing = fetch_california_housing()\n",
            "\n",
            "    for the California housing dataset and::\n",
            "\n",
            "        from sklearn.datasets import fetch_openml\n",
            "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
            "\n",
            "    for the Ames housing dataset.\n",
            "    \n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss after mini-batch     1: 0.043\n",
            "Loss after mini-batch    11: 0.426\n",
            "Loss after mini-batch    21: 0.442\n",
            "Loss after mini-batch    31: 0.461\n",
            "Loss after mini-batch    41: 0.452\n",
            "Loss after mini-batch    51: 0.472\n",
            "Starting epoch 2\n",
            "Loss after mini-batch     1: 0.047\n",
            "Loss after mini-batch    11: 0.432\n",
            "Loss after mini-batch    21: 0.450\n",
            "Loss after mini-batch    31: 0.434\n",
            "Loss after mini-batch    41: 0.485\n",
            "Loss after mini-batch    51: 0.434\n",
            "Starting epoch 3\n",
            "Loss after mini-batch     1: 0.049\n",
            "Loss after mini-batch    11: 0.458\n",
            "Loss after mini-batch    21: 0.435\n",
            "Loss after mini-batch    31: 0.423\n",
            "Loss after mini-batch    41: 0.417\n",
            "Loss after mini-batch    51: 0.480\n",
            "Starting epoch 4\n",
            "Loss after mini-batch     1: 0.051\n",
            "Loss after mini-batch    11: 0.436\n",
            "Loss after mini-batch    21: 0.417\n",
            "Loss after mini-batch    31: 0.452\n",
            "Loss after mini-batch    41: 0.447\n",
            "Loss after mini-batch    51: 0.438\n",
            "Starting epoch 5\n",
            "Loss after mini-batch     1: 0.043\n",
            "Loss after mini-batch    11: 0.442\n",
            "Loss after mini-batch    21: 0.427\n",
            "Loss after mini-batch    31: 0.428\n",
            "Loss after mini-batch    41: 0.424\n",
            "Loss after mini-batch    51: 0.451\n",
            "Training process has finished.\n"
          ]
        }
      ]
    }
  ]
}