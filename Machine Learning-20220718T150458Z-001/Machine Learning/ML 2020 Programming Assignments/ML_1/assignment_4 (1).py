# -*- coding: utf-8 -*-
"""Assignment_4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A3g61D505qpMVLCNaQFB8OUQDqzsp1ot

# **Install necessary libraries**
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install tensorflow keras numpy mnist matplotlib
import numpy as np
import pandas as pd
import mnist
import matplotlib.pyplot as plt
from tensorflow import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.utils import to_categorical
# %matplotlib inline

"""# **Split the dataset into train and test**"""

X_train=mnist.train_images()
Y_train=mnist.train_labels()
X_test=mnist.test_images()
Y_test=mnist.test_labels()

"""# **Normalize pixels[0,255] into [-0.5,0.5] and Reshaping it**"""

X_train= (X_train/255)-0.5
X_test= (X_test/255)-0.5
X_train= X_train.reshape(-1,784)
X_test= X_test.reshape(-1,784)

"""## **Building model**"""

# Adding 2 layers with 64 neurons each with activation function=relu
# 1 layer with 10 neurons with softmax function
model = Sequential()
model.add(Dense(64, input_dim=784, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(10, activation='softmax'))

"""# **Compiling and Fitting(backround black and digit white) with three batches 1000 and 500 and 200 size in sequence**"""

# Adam optimizer improves the model 
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
h=model.fit(X_train, to_categorical(Y_train),validation_split=0.16,batch_size= 1000, epochs =8,verbose = 1)

"""**Evaluate the model**"""

s=model.evaluate(X_test, to_categorical(Y_test))
print("accuracy: ", s[1])

"""**Predict from the model**"""

pred=model.predict( X_test[9:27])
print(np.argmax(pred,axis=1))
print(Y_test[9:27])

"""**Image display**"""

pix=X_test[27].reshape(28,28)
plt.imshow(pix,cmap='gray')
plt.show()

"""**Loss v/s epoch graph**"""

plt.plot(h.history['loss'])
plt.plot(h.history['val_loss'])
plt.title('model loss with batch size 1000')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()

# batch size=500
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
h=model.fit(X_train, to_categorical(Y_train),validation_split=0.16,batch_size= 500, epochs =8,verbose = 1)

plt.plot(h.history['loss'])
plt.plot(h.history['val_loss'])
plt.title('model loss with batch size 500')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()

# batch size=200
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
history=model.fit(X_train, to_categorical(Y_train),validation_split=0.16,batch_size= 200, epochs =8,verbose = 1)

plt.plot(h.history['loss'])
plt.plot(h.history['val_loss'])
plt.title('model loss with batch size 200')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()

"""# **Compiling and Fitting(50% original and 50% flipped) with three batches 1000 and 500 and 200 size in sequence**"""

#creating new train and test dataset

train_X=[]
for i in range(X_train.shape[0]):
  if (i<30000):
    train_X.append(X_train[i])
  else:
    train_X.append(-X_train[i])
train_X=np.array(train_X)
test_X=[]
for i in range(X_test.shape[0]):
  if (i<30000):
    test_X.append(X_test[i])
  else:
    test_X.append(-X_test[i])
test_X=np.array(test_X)

# Adam optimizer improves the model 
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
h=model.fit(train_X, to_categorical(Y_train),validation_split=0.16,batch_size= 1000, epochs =8,verbose = 0)

"""**Evaluate the model**"""

s=model.evaluate(test_X, to_categorical(Y_test))
print("accuracy: ", s[1])

"""**Pedict from the model**"""

pred=model.predict(test_X[9:27])
print(np.argmax(pred,axis=1))
print(Y_test[9:27])

"""**Loss v/s epoch graph**"""

plt.plot(h.history['loss'])
plt.plot(h.history['val_loss'])
plt.title('model loss with batch size 1000')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()

# batch size 500
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
h=model.fit(train_X, to_categorical(Y_train),validation_split=0.16,batch_size= 500, epochs =8,verbose = 1)

plt.plot(h.history['loss'])
plt.plot(h.history['val_loss'])
plt.title('model loss with batch size 500')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['t', 'tw'], loc='upper right')
plt.show()

# Batch size 200
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
h=model.fit(train_X, to_categorical(Y_train),validation_split=0.16,batch_size= 200, epochs =8,verbose = 1)

plt.plot(h.history['loss'])
plt.plot(h.history['val_loss'])
plt.title('model loss with batch size 200')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()

#Conclusion: As the batch size increases, accuracy increases while training with original images
#After mixing the flipped and original images with equal no, the accuracy decreased slightly.